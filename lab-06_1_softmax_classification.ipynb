{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6-1: Softmax Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Seungjae Lee (이승재)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    We use elemental PyTorch to implement linear regression here. However, in most actual applications, abstractions such as <code>nn.Module</code> or <code>nn.Linear</code> are used.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2a2004a9b0>"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert numbers to probabilities with softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(class=i) = \\frac{e^i}{\\sum e^i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.FloatTensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has a `softmax` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2877, 0.4874, 0.3291, 0.2828, 0.2280],\n",
      "        [0.3284, 0.2735, 0.2664, 0.4154, 0.3707],\n",
      "        [0.3839, 0.2391, 0.4045, 0.3018, 0.4013]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = F.softmax(z, dim=0)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since they are probabilities, they should add up to 1. Let's do a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss (Low-level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multi-class classification, we use the cross entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ L = \\frac{1}{N} \\sum - y \\log(\\hat{y}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\hat{y}$ is the predicted probability and $y$ is the correct probability (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0169, 0.3830, 0.8878, 0.7163, 0.2505],\n",
      "        [0.0670, 0.4946, 0.6932, 0.0849, 0.0816],\n",
      "        [0.4937, 0.3975, 0.8214, 0.0155, 0.4205]], requires_grad=True)\n",
      "tensor([[0.1233, 0.1779, 0.2947, 0.2483, 0.1558],\n",
      "        [0.1554, 0.2383, 0.2906, 0.1582, 0.1576],\n",
      "        [0.2064, 0.1874, 0.2864, 0.1279, 0.1918]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "z = torch.rand(3, 5, requires_grad=True)\n",
    "print(z)\n",
    "hypothesis = F.softmax(z, dim=1)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 4, 0])\n"
     ]
    }
   ],
   "source": [
    "y = torch.randint(5, (3,)).long()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot = torch.zeros_like(hypothesis)\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_one_hot\n",
      " tensor([[0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0.]]) \n",
      "\n",
      "hypothesis\n",
      " tensor([[0.1233, 0.1779, 0.2947, 0.2483, 0.1558],\n",
      "        [0.1554, 0.2383, 0.2906, 0.1582, 0.1576],\n",
      "        [0.2064, 0.1874, 0.2864, 0.1279, 0.1918]], grad_fn=<SoftmaxBackward>) \n",
      "\n",
      "-torch.log(hypothesis)\n",
      " tensor([[2.0927, 1.7266, 1.2218, 1.3933, 1.8591],\n",
      "        [1.8620, 1.4344, 1.2359, 1.8441, 1.8474],\n",
      "        [1.5780, 1.6743, 1.2503, 2.0562, 1.6512]], grad_fn=<NegBackward>) \n",
      "\n",
      "y_one_hot * -torch.log(hypothesis)\n",
      " tensor([[0.0000, 1.7266, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.8474],\n",
      "        [1.5780, 0.0000, 0.0000, 0.0000, 0.0000]], grad_fn=<MulBackward0>) \n",
      "\n",
      "(y_one_hot * -torch.log(hypothesis)).sum(dim=1)\n",
      " tensor([1.7266, 1.8474, 1.5780], grad_fn=<SumBackward2>) \n",
      "\n",
      "tensor(1.7173, grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(\"y_one_hot\\n\", y_one_hot, \"\\n\")\n",
    "print(\"hypothesis\\n\", hypothesis, \"\\n\")\n",
    "print(\"-torch.log(hypothesis)\\n\", -torch.log(hypothesis), \"\\n\")\n",
    "print(\"y_one_hot * -torch.log(hypothesis)\\n\", y_one_hot * -torch.log(hypothesis), \"\\n\")\n",
    "print(\"(y_one_hot * -torch.log(hypothesis)).sum(dim=1)\\n\", \n",
    "      (y_one_hot * -torch.log(hypothesis)).sum(dim=1), \"\\n\")\n",
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy Loss with `torch.nn.functional`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has `F.log_softmax()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0927, -1.7266, -1.2218, -1.3933, -1.8591],\n",
       "        [-1.8620, -1.4344, -1.2359, -1.8441, -1.8474],\n",
       "        [-1.5780, -1.6743, -1.2503, -2.0562, -1.6512]], grad_fn=<LogBackward>)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Low level\n",
    "torch.log(F.softmax(z, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0927, -1.7266, -1.2218, -1.3933, -1.8591],\n",
       "        [-1.8620, -1.4344, -1.2359, -1.8441, -1.8474],\n",
       "        [-1.5780, -1.6743, -1.2503, -2.0562, -1.6512]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High level\n",
    "F.log_softmax(z, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch also has `F.nll_loss()` function that computes the negative loss likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7173, grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Low level\n",
    "(y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7173, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High level\n",
    "F.nll_loss(F.log_softmax(z, dim=1), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch also has `F.cross_entropy` that combines `F.log_softmax()` and `F.nll_loss()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7173, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Low-level Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.901535\n",
      "Epoch  200/1000 Cost: 0.839114\n",
      "Epoch  300/1000 Cost: 0.807826\n",
      "Epoch  400/1000 Cost: 0.788472\n",
      "Epoch  500/1000 Cost: 0.774822\n",
      "Epoch  600/1000 Cost: 0.764449\n",
      "Epoch  700/1000 Cost: 0.756191\n",
      "Epoch  800/1000 Cost: 0.749398\n",
      "Epoch  900/1000 Cost: 0.743671\n",
      "Epoch 1000/1000 Cost: 0.738749\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # Cost 계산 (1)\n",
    "    hypothesis = F.softmax(x_train.matmul(W) + b, dim=1) # or .mm or @\n",
    "    y_one_hot = torch.zeros_like(hypothesis)\n",
    "    y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "    cost = (y_one_hot * -torch.log(F.softmax(hypothesis, dim=1))).sum(dim=1).mean()\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkAccuracy(W, b):\n",
    "    print(W)\n",
    "    print(b)\n",
    "    hypothesis = F.softmax(x_train.matmul(W) + b, dim=1)\n",
    "    print(hypothesis)\n",
    "    print(\"----------------\")\n",
    "    res = torch.max(hypothesis, dim=1)[1]\n",
    "    print(res)\n",
    "    print(y_train)\n",
    "    print(\"----------------\")    \n",
    "    match = 0\n",
    "    for i in range(8):\n",
    "        if res[i] == y_train[i]:\n",
    "            match += 1\n",
    "    print(match, \"/\", y_train.size()[0])\n",
    "    print(match / y_train.size()[0] * 100, \"% accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6429, -0.8499,  2.4928],\n",
      "        [ 0.7231, -2.2532,  1.5301],\n",
      "        [ 0.2087,  0.8192, -1.0279],\n",
      "        [-0.0671,  0.9278, -0.8607]], requires_grad=True)\n",
      "tensor([-2.1531e-08], requires_grad=True)\n",
      "tensor([[2.3658e-02, 6.7666e-04, 9.7566e-01],\n",
      "        [1.7783e-02, 2.0217e-01, 7.8005e-01],\n",
      "        [1.3411e-03, 2.4644e-01, 7.5222e-01],\n",
      "        [1.9756e-04, 7.3565e-01, 2.6416e-01],\n",
      "        [5.9065e-01, 3.5765e-06, 4.0935e-01],\n",
      "        [2.0591e-02, 9.7929e-01, 1.1413e-04],\n",
      "        [9.6044e-01, 5.6807e-04, 3.8996e-02],\n",
      "        [9.8805e-01, 1.4835e-04, 1.1805e-02]], grad_fn=<SoftmaxBackward>)\n",
      "----------------\n",
      "tensor([2, 2, 2, 1, 0, 1, 0, 0])\n",
      "tensor([2, 2, 2, 1, 1, 1, 0, 0])\n",
      "----------------\n",
      "7 / 8\n",
      "87.5 % accuracy\n"
     ]
    }
   ],
   "source": [
    "checkAccuracy(W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with `F.cross_entropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.761050\n",
      "Epoch  200/1000 Cost: 0.689991\n",
      "Epoch  300/1000 Cost: 0.643229\n",
      "Epoch  400/1000 Cost: 0.604117\n",
      "Epoch  500/1000 Cost: 0.568255\n",
      "Epoch  600/1000 Cost: 0.533922\n",
      "Epoch  700/1000 Cost: 0.500291\n",
      "Epoch  800/1000 Cost: 0.466908\n",
      "Epoch  900/1000 Cost: 0.433507\n",
      "Epoch 1000/1000 Cost: 0.399962\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # Cost 계산 (2)\n",
    "    z = x_train.matmul(W) + b # or .mm or @\n",
    "    cost = F.cross_entropy(z, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-10.2105,  -1.1448,  11.3551],\n",
      "        [ -2.8180,  -1.5091,   4.3270],\n",
      "        [  8.7516,  -1.7761,  -6.9756],\n",
      "        [ -2.7641,   4.5773,  -1.8133]], requires_grad=True)\n",
      "tensor([-3.6930e-06], requires_grad=True)\n",
      "tensor([[7.0047e-10, 3.4334e-06, 1.0000e+00],\n",
      "        [6.1901e-03, 7.8503e-02, 9.1531e-01],\n",
      "        [3.9055e-13, 1.0204e-01, 8.9796e-01],\n",
      "        [3.5657e-10, 8.9235e-01, 1.0765e-01],\n",
      "        [9.0537e-02, 9.0039e-01, 9.0690e-03],\n",
      "        [4.3358e-02, 9.5664e-01, 3.4321e-18],\n",
      "        [9.0007e-01, 9.9925e-02, 2.7208e-11],\n",
      "        [9.8330e-01, 1.6703e-02, 1.4417e-14]], grad_fn=<SoftmaxBackward>)\n",
      "----------------\n",
      "tensor([2, 2, 2, 1, 1, 1, 0, 0])\n",
      "tensor([2, 2, 2, 1, 1, 1, 0, 0])\n",
      "----------------\n",
      "8 / 8\n",
      "100.0 % accuracy\n"
     ]
    }
   ],
   "source": [
    "checkAccuracy(W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level Implementation with `nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 3) # Output이 3!\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 2.637636\n",
      "Epoch  100/1000 Cost: 0.647903\n",
      "Epoch  200/1000 Cost: 0.564643\n",
      "Epoch  300/1000 Cost: 0.511043\n",
      "Epoch  400/1000 Cost: 0.467249\n",
      "Epoch  500/1000 Cost: 0.428281\n",
      "Epoch  600/1000 Cost: 0.391924\n",
      "Epoch  700/1000 Cost: 0.356742\n",
      "Epoch  800/1000 Cost: 0.321577\n",
      "Epoch  900/1000 Cost: 0.285617\n",
      "Epoch 1000/1000 Cost: 0.250818\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.2894, -0.4193,  4.5199],\n",
      "        [-2.9419,  0.4789,  2.5343],\n",
      "        [-7.0699,  3.5619,  4.7770],\n",
      "        [-6.1064,  4.2210,  3.1405],\n",
      "        [ 0.9614,  1.0687, -2.1034],\n",
      "        [ 2.3790,  3.5190, -3.9502],\n",
      "        [ 3.1379,  1.9785, -4.5445],\n",
      "        [ 4.5466,  2.4102, -6.2876]], grad_fn=<AddmmBackward>)\n",
      "----------------\n",
      "tensor([2, 2, 2, 1, 1, 1, 0, 0])\n",
      "tensor([2, 2, 2, 1, 1, 1, 0, 0])\n",
      "----------------\n",
      "8 / 8\n",
      "100.0 % accuracy\n"
     ]
    }
   ],
   "source": [
    "hypothesis = model.forward(x_train)\n",
    "print(hypothesis)\n",
    "print(\"----------------\")\n",
    "res = torch.max(hypothesis, dim=1)[1]\n",
    "print(res)\n",
    "print(y_train)\n",
    "print(\"----------------\")    \n",
    "match = 0\n",
    "for i in range(8):\n",
    "    if res[i] == y_train[i]:\n",
    "        match += 1\n",
    "print(match, \"/\", y_train.size()[0])\n",
    "print(match / y_train.size()[0] * 100, \"% accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
